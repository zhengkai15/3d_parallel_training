# 大型模型配置 (类似GPT-2 Large / GPT-3 1.3B)
# 需要多节点集群

model:
  vocab_size: 50000
  hidden_size: 2048
  num_layers: 48
  num_heads: 32
  max_seq_len: 2048
  ffn_hidden_size: 8192  # 4 * hidden_size
  attention_dropout: 0.1
  hidden_dropout: 0.1

training:
  num_samples: 1000000
  batch_size: 2
  gradient_accumulation_steps: 16
  max_steps: 100000
  learning_rate: 1.5e-4
  min_lr: 1.5e-5
  weight_decay: 0.1
  warmup_steps: 5000
  max_grad_norm: 1.0
  fp16: true

parallel:
  tp_size: 4
  pp_size: 4
  # 建议使用32+ GPUs
  # dp_size = world_size / (4 * 4) = world_size / 16

logging:
  logging_steps: 10
  save_steps: 5000
  eval_steps: 2000
  output_dir: "./output_large"

# 3Då¹¶è¡Œè®­ç»ƒæ¡†æ¶

<div align="center">

**å·¥ä¸šçº§å¤§è¯­è¨€æ¨¡å‹3Då¹¶è¡Œè®­ç»ƒç³»ç»Ÿ**

æ”¯æŒ Data Parallel (DP) + Tensor Parallel (TP) + Pipeline Parallel (PP)

[å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹) â€¢ [æ–‡æ¡£](docs/USAGE_GUIDE.md) â€¢ [ç¤ºä¾‹](examples/)

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

---

## âœ¨ æ ¸å¿ƒç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ |
|-----|------|
| ğŸ¯ **å®Œæ•´3Då¹¶è¡Œ** | DP + TP (Megatron) + PP (GPipe/1F1B) |
| âš¡ **é«˜æ€§èƒ½** | æ¥è¿‘ç†è®ºåŠ é€Ÿæ¯”ï¼Œæ”¯æŒæ··åˆç²¾åº¦ |
| ğŸ”§ **æ˜“ç”¨æ€§** | ç»Ÿä¸€å…¥å£è„šæœ¬ï¼Œä¸€é”®å¯åŠ¨ |
| ğŸ“¦ **å¼€ç®±å³ç”¨** | é¢„é…ç½®æ¨¡å‹ï¼Œè‡ªåŠ¨ä¼˜åŒ– |
| ğŸŒ **å¤šèŠ‚ç‚¹** | æ”¯æŒå•æœºå¤šå¡å’Œå¤šæœºå¤šå¡ |
| ğŸ” **å¯è§‚æµ‹** | å®Œæ•´æ—¥å¿—ã€ç›‘æ§ã€æ£€æŸ¥ç‚¹ |

## ğŸ“ é¡¹ç›®ç»“æ„

```
3d_parallel_training/
â”œâ”€â”€ train                      # ğŸ¯ ç»Ÿä¸€å…¥å£è„šæœ¬ (æ¨è)
â”œâ”€â”€ train.py                   # æ ‡å‡†è®­ç»ƒè„šæœ¬
â”œâ”€â”€ train_3d_parallel.py       # 3Då¹¶è¡Œè®­ç»ƒ
â”œâ”€â”€ train_megatron.py          # Megatronè®­ç»ƒ
â”‚
â”œâ”€â”€ model.py                   # åŸºç¡€æ¨¡å‹
â”œâ”€â”€ megatron_model.py          # Megatronæ¨¡å‹
â”œâ”€â”€ pipeline_parallel.py       # Pipelineå¼•æ“
â”‚
â”œâ”€â”€ scripts/                   # å¯åŠ¨è„šæœ¬
â”‚   â”œâ”€â”€ run_3d_parallel.sh
â”‚   â”œâ”€â”€ run_megatron.sh
â”‚   â”œâ”€â”€ run_deepspeed.sh
â”‚   â””â”€â”€ run_multinode.sh
â”‚
â”œâ”€â”€ configs/                   # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ models/                # æ¨¡å‹é…ç½®
â”‚   â”‚   â”œâ”€â”€ small.yaml
â”‚   â”‚   â”œâ”€â”€ medium.yaml
â”‚   â”‚   â””â”€â”€ large.yaml
â”‚   â””â”€â”€ deepspeed/             # DeepSpeedé…ç½®
â”‚       â”œâ”€â”€ zero2.json
â”‚       â””â”€â”€ zero3.json
â”‚
â”œâ”€â”€ tools/                     # å·¥å…·
â”‚   â”œâ”€â”€ monitor.py
â”‚   â”œâ”€â”€ test_model.py
â”‚   â”œâ”€â”€ benchmark.sh
â”‚   â””â”€â”€ quick_test.sh
â”‚
â”œâ”€â”€ docs/                      # æ–‡æ¡£
â”‚   â””â”€â”€ USAGE_GUIDE.md
â”‚
â””â”€â”€ examples/                  # ç¤ºä¾‹ (TODO)
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# å®‰è£…PyTorch (æ ¹æ®CUDAç‰ˆæœ¬é€‰æ‹©)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…å…¶ä»–ä¾èµ–
pip install -r requirements.txt

# å¿«é€Ÿæµ‹è¯•
bash tools/quick_test.sh
```

### 2. é€‰æ‹©è®­ç»ƒæ¨¡å¼

#### æ–¹å¼A: ä½¿ç”¨ç»Ÿä¸€å…¥å£ (æ¨è â­)

```bash
# ç®€å•DDPè®­ç»ƒ (å…¥é—¨)
python train --mode simple --gpus 4

# 3Då¹¶è¡Œè®­ç»ƒ (æ¨è)
python train --mode 3d --gpus 8 --tp 2 --pp 2

# Megatronè®­ç»ƒ (é«˜æ€§èƒ½)
python train --mode megatron --gpus 8

# DeepSpeedè®­ç»ƒ (çœå†…å­˜)
python train --mode deepspeed --gpus 4 --zero-stage 2
```

#### æ–¹å¼B: ä½¿ç”¨Shellè„šæœ¬

```bash
# 3Då¹¶è¡Œ
NUM_GPUS=8 TP_SIZE=2 PP_SIZE=2 bash scripts/run_3d_parallel.sh

# Megatron
NUM_GPUS=8 bash scripts/run_megatron.sh

# DeepSpeed
ZERO_STAGE=2 bash scripts/run_deepspeed.sh
```

#### æ–¹å¼C: ç›´æ¥è°ƒç”¨Pythonè„šæœ¬

```bash
# 3Då¹¶è¡Œ
torchrun --nproc_per_node=8 train_3d_parallel.py \
    --tp_size 2 --pp_size 2 --hidden_size 768

# Megatron
torchrun --nproc_per_node=8 train_megatron.py \
    --hidden_size 1024 --num_layers 24
```

### 3. ç›‘æ§è®­ç»ƒ

```bash
# å®æ—¶ç›‘æ§
python tools/monitor.py

# æŸ¥çœ‹æ—¥å¿—
tail -f output_*/train.log

# GPUçŠ¶æ€
watch -n 1 nvidia-smi
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| é…ç½® | GPUæ•° | åŠ é€Ÿæ¯” | é€‚ç”¨æ¨¡å‹ |
|-----|-------|--------|---------|
| DDP | 4 | 3.5x | < 1B |
| DP+TP | 4 | 3.2x | 1-3B |
| DP+TP+PP | 8 | 6.5x | 3-10B |
| 3Då¹¶è¡Œ | 16 | 12x | 10-30B |
| 3D+ZeRO3 | 32 | 22x | 30-100B |

```bash
# è¿è¡Œæ€§èƒ½æµ‹è¯•
NUM_GPUS=8 bash tools/benchmark.sh
```

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: å°æ¨¡å‹å¿«é€Ÿè®­ç»ƒ

```bash
# GPT-2 Small (117Må‚æ•°)
python train --mode simple --gpus 4 \
    --hidden-size 768 --num-layers 12 \
    --batch-size 8 --max-steps 1000
```

### ç¤ºä¾‹2: ä¸­å‹æ¨¡å‹è®­ç»ƒ

```bash
# GPT-2 Medium (345Må‚æ•°)
python train --mode 3d --gpus 8 --tp 2 --pp 2 \
    --hidden-size 1024 --num-layers 24 \
    --batch-size 4 --max-steps 10000
```

### ç¤ºä¾‹3: å¤§æ¨¡å‹è®­ç»ƒ

```bash
# GPT-3 1.3B
python train --mode megatron --gpus 16 \
    --config configs/models/large.yaml
```

### ç¤ºä¾‹4: å¤šèŠ‚ç‚¹è®­ç»ƒ

**èŠ‚ç‚¹0 (ä¸»èŠ‚ç‚¹)**:
```bash
NUM_NODES=2 NODE_RANK=0 MASTER_ADDR="192.168.1.100" \
bash scripts/run_multinode.sh
```

**èŠ‚ç‚¹1**:
```bash
NUM_NODES=2 NODE_RANK=1 MASTER_ADDR="192.168.1.100" \
bash scripts/run_multinode.sh
```

## âš™ï¸ é…ç½®æŒ‡å—

### å¹¶è¡Œç­–ç•¥é€‰æ‹©

```python
# å†³ç­–æ ‘
if æ¨¡å‹ < 1B:
    ä½¿ç”¨ DDP (--mode simple)
elif æ¨¡å‹ < 10B:
    ä½¿ç”¨ DP+TP (--mode 3d --tp 2)
elif æ¨¡å‹ < 100B:
    ä½¿ç”¨ DP+TP+PP (--mode 3d --tp 4 --pp 2)
else:
    ä½¿ç”¨ 3D+ZeRO3 (--mode deepspeed --zero-stage 3)
```

### å‚æ•°å»ºè®®

| å‚æ•° | å°æ¨¡å‹ | ä¸­æ¨¡å‹ | å¤§æ¨¡å‹ |
|-----|--------|--------|--------|
| `--tp` | 1 | 2 | 4-8 |
| `--pp` | 1 | 2 | 2-4 |
| `--batch-size` | 8-16 | 4-8 | 2-4 |
| `--zero-stage` | 0 | 2 | 3 |

## ğŸ“– æ–‡æ¡£

- **[USAGE_GUIDE.md](docs/USAGE_GUIDE.md)** - å®Œæ•´ä½¿ç”¨æŒ‡å—
  - è¯¦ç»†çš„å¹¶è¡Œç­–ç•¥è¯´æ˜
  - å¤šèŠ‚ç‚¹è®­ç»ƒé…ç½®
  - æ€§èƒ½ä¼˜åŒ–æŠ€å·§
  - å¸¸è§é—®é¢˜è§£ç­”

## ğŸ› ï¸ é«˜çº§åŠŸèƒ½

### æ··åˆç²¾åº¦è®­ç»ƒ

```bash
python train --mode 3d --gpus 8 --fp16
```

### è‡ªå®šä¹‰é…ç½®æ–‡ä»¶

```bash
python train --mode megatron --config my_config.yaml
```

### ä»æ£€æŸ¥ç‚¹æ¢å¤

```bash
python train --mode 3d --resume-from output/checkpoint-1000
```

## ğŸ› æ•…éšœæ’æŸ¥

### OOM (å†…å­˜ä¸è¶³)

```bash
# æ–¹æ¡ˆ1: å‡å°batch size
python train --mode 3d --batch-size 2

# æ–¹æ¡ˆ2: ä½¿ç”¨ZeRO-3
python train --mode deepspeed --zero-stage 3

# æ–¹æ¡ˆ3: å¢åŠ å¹¶è¡Œåº¦
python train --mode 3d --tp 4 --pp 2
```

### è®­ç»ƒé€Ÿåº¦æ…¢

```bash
# æ–¹æ¡ˆ1: æ£€æŸ¥GPUåˆ©ç”¨ç‡
nvidia-smi dmon

# æ–¹æ¡ˆ2: å¢å¤§batch size
python train --batch-size 16

# æ–¹æ¡ˆ3: ä½¿ç”¨æ··åˆç²¾åº¦
python train --fp16
```

### é€šä¿¡è¶…æ—¶

```bash
# å¢åŠ è¶…æ—¶æ—¶é—´
export NCCL_TIMEOUT=3600

# å¯ç”¨è°ƒè¯•
export NCCL_DEBUG=INFO
```

## ğŸ“ è·å–å¸®åŠ©

```bash
# æŸ¥çœ‹å¸®åŠ©
python train --help

# æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£
cat docs/USAGE_GUIDE.md

# è¿è¡Œæµ‹è¯•
bash tools/quick_test.sh
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼è¯·æŸ¥çœ‹ [CONTRIBUTING.md](CONTRIBUTING.md)

## ğŸ“„ è®¸å¯è¯

MIT License - è¯¦è§ [LICENSE](LICENSE)

## ğŸ™ è‡´è°¢

æœ¬é¡¹ç›®å‚è€ƒäº†ï¼š
- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
- [DeepSpeed](https://github.com/microsoft/DeepSpeed)
- [PyTorch](https://pytorch.org/)

---

<div align="center">

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ªStarï¼â­**

Made with â¤ï¸ for the AI community

</div>
